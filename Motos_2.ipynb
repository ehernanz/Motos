{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escribimos_excel(in_lista_datos):\n",
    "        print(\"       Instanciamos el ficehro de salida\")        \n",
    "        #Escribimos el fichero de salida\n",
    "        workbook = xlsxwriter.Workbook(\"Datos_Motos_\"+time.strftime(\"%d_%m_%Y\")+\".xlsx\")\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        # Start from the first cell. Rows and columns are zero indexed.\n",
    "        row = 1\n",
    "        col = 0\n",
    "        worksheet.write(0, col,     \"Modelo\")\n",
    "        worksheet.write(0, col + 1, \"anio\")\n",
    "        worksheet.write(0, col + 2, \"km\")\n",
    "        worksheet.write(0, col + 3, \"provincia\")\n",
    "        worksheet.write(0, col + 4, \"cilindrada\")\n",
    "        worksheet.write(0, col + 5, \"url\")\n",
    "        worksheet.write(0, col + 6, \"Preio\")\n",
    "        worksheet.write(0, col + 7, \"fec_publicaion\")\n",
    "        worksheet.write(0, col + 8, \"Comentario\")\n",
    "        worksheet.write(0, col + 9, \"pagina\")\n",
    "\n",
    "        print(\"       Iniciamos el recorrido del Loop\")\n",
    "        \n",
    "        # Iterate over the data and write it out row by row.\n",
    "        for modelo,anio,km,provincia,cilindrada,url,preio,fec_publicaion,comentario,pagina in (in_lista_datos):\n",
    "            worksheet.write(row, col,     modelo)\n",
    "            worksheet.write(row, col + 1, int(anio))\n",
    "            worksheet.write(row, col + 2, int(\"0\"+km.replace(\".\",\"\").replace(\" km\",\"\").replace(\"N/D\",\"\").strip()))\n",
    "            worksheet.write(row, col + 3, provincia)\n",
    "            worksheet.write(row, col + 4, cilindrada)\n",
    "            worksheet.write(row, col + 5, \"http://motos.coches.net\"+url)\n",
    "            worksheet.write(row, col + 6, int(\"0\"+preio.replace(\".\",\"\").replace(\" €\",\"\").replace(\"N/D\",\"\").strip()))\n",
    "            worksheet.write(row, col + 7, fec_publicaion)\n",
    "            worksheet.write(row, col + 8, comentario)\n",
    "            worksheet.write(row, col + 9, pagina)\n",
    "\n",
    "            row += 1\n",
    "        num_filas = len(in_lista_datos)\n",
    "        print(\"Guardando.... %d  filas\" % num_filas)\n",
    "        workbook.close()\n",
    "        return \"FIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_comentarios(dir_url):\n",
    "    #print(\"      Obtenemos comentarios del anuncio\")\n",
    "    # spoof some headers so the request appears to be coming from a browser, not a bot\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5)\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"accept-charset\": \"ISO-8859-1,utf-8;q=0.7,*;q=0.3\",\n",
    "        \"accept-encoding\": \"gzip,deflate,sdch\",\n",
    "        \"accept-language\": \"en-US,en;q=0.8\",\n",
    "        }\n",
    "\n",
    "    # make the request to the search url, passing in the the spoofed headers.\n",
    "    r = requests.get(dir_url, headers=headers)  # assign the response to a variable r\n",
    "\n",
    "    # check the status code of the response to make sure the request went well\n",
    "    if r.status_code != 200:\n",
    "        print(\"request denied\")\n",
    "        print(r.status_code)\n",
    "        sigue = 0\n",
    "\n",
    "    #else:\n",
    "        #print(\"scraping \" + url)  \n",
    "\n",
    "\n",
    "    # convert the plaintext HTML markup into a DOM-like structure that we can search\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')# each result is an <li> element with class=\"g\" this is our wrapper\n",
    "\n",
    "    results = soup.findAll(\"div\", attrs={\"id\":\"div_comentarios\"})\n",
    "\n",
    "    #print(results)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        #print(\"No hay comentarios\")\n",
    "        comentario = \"\"\n",
    "\n",
    "    else:\n",
    "        # iterate over each of the result wrapper elements\n",
    "        comentario = results[0].findAll(\"p\")[0].text.strip()\n",
    "        \n",
    "    #print(\"Comentario:  %s\" % comentario)\n",
    "    return comentario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_motos():\n",
    "\n",
    "        pagina = 0  \n",
    "        sigue = 1\n",
    "\n",
    "        lista_datos = []\n",
    "\n",
    "        while sigue == 1:\n",
    "            \n",
    "            pagina += 1\n",
    "            print(\"Pagina: %s\" % pagina)\n",
    "          \n",
    "            # dynamically build the URL that we'll be making a request to\n",
    "            url = \"http://motos.coches.net/ocasion/default.aspx?pg=\"+str(pagina)+\"&fi=CreationDate\"\n",
    "            # spoof some headers so the request appears to be coming from a browser, not a bot\n",
    "            headers = {\n",
    "                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_5)\",\n",
    "                \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "                \"accept-charset\": \"ISO-8859-1,utf-8;q=0.7,*;q=0.3\",\n",
    "                \"accept-encoding\": \"gzip,deflate,sdch\",\n",
    "                \"accept-language\": \"en-US,en;q=0.8\",\n",
    "                }\n",
    "\n",
    "            # make the request to the search url, passing in the the spoofed headers.\n",
    "            r = requests.get(url, headers=headers)  # assign the response to a variable r\n",
    "\n",
    "            # check the status code of the response to make sure the request went well\n",
    "            if r.status_code != 200:\n",
    "                print(\"request denied\")\n",
    "                print(r.status_code)\n",
    "                sigue = 0\n",
    "\n",
    "            #else:\n",
    "                #print(\"scraping \" + url)  \n",
    "\n",
    "\n",
    "            # convert the plaintext HTML markup into a DOM-like structure that we can search\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')# each result is an <li> element with class=\"g\" this is our wrapper\n",
    "\n",
    "            results = soup.findAll(\"a\", attrs={\"class\":\"lnkad\"})\n",
    "\n",
    "            #Para pruebas\n",
    "            #if len(results) == 0 or pagina == 2:\n",
    "            \n",
    "            if len(results) == 0:\n",
    "                sigue = 0\n",
    "                print(\"Paramos de recuperar páginas, la última no tiene datos\")\n",
    "\n",
    "            else:\n",
    "                # iterate over each of the result wrapper elements\n",
    "                for result in results:\n",
    "                    modelo = result.attrs[\"title\"]\n",
    "                    url=result.attrs[\"href\"]\n",
    "\n",
    "                    preio = result.findAll(\"p\", attrs={\"class\":\"preu\"})[0].text\n",
    "                    fec_publicaion = result.findAll(\"p\", attrs={\"class\":\"data floatright\"})[0].text\n",
    "\n",
    "                    datos = result.findAll(\"p\", attrs={\"class\":\"dades\"})[0]\n",
    "\n",
    "                    #print(datos)\n",
    "                    km = datos.findAll(\"span\", attrs={\"class\":\"d1\"})[0].text\n",
    "                    anio = datos.findAll(\"span\", attrs={\"class\":\"d2\"})[0].text  \n",
    "                    cilindrada = datos.findAll(\"span\", attrs={\"class\":\"d3\"})[0].text\n",
    "                    provincia = datos.findAll(\"span\", attrs={\"class\":\"lloc\"})[0].text\n",
    "\n",
    "                    comentario=obtener_comentarios(\"http://motos.coches.net\"+url)\n",
    "                    \n",
    "                    \n",
    "                    lista_datos.append([modelo,anio,km,provincia,cilindrada,url,preio,fec_publicaion,comentario,pagina])\n",
    "\n",
    "        return lista_datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciamos Scrapeo\n",
      "Pagina: 1\n",
      "Pagina: 2\n",
      "Pagina: 3\n",
      "Pagina: 4\n",
      "Pagina: 5\n",
      "Pagina: 6\n",
      "Pagina: 7\n",
      "Pagina: 8\n",
      "Pagina: 9\n",
      "Pagina: 10\n",
      "Pagina: 11\n",
      "Pagina: 12\n",
      "Pagina: 13\n",
      "Pagina: 14\n",
      "Pagina: 15\n",
      "Pagina: 16\n",
      "Pagina: 17\n",
      "Pagina: 18\n",
      "Pagina: 19\n",
      "Pagina: 20\n",
      "Pagina: 21\n",
      "Pagina: 22\n",
      "Pagina: 23\n",
      "Pagina: 24\n",
      "Pagina: 25\n",
      "Pagina: 26\n",
      "Pagina: 27\n",
      "Pagina: 28\n",
      "Pagina: 29\n",
      "Pagina: 30\n",
      "Pagina: 31\n",
      "Pagina: 32\n",
      "Pagina: 33\n",
      "Pagina: 34\n",
      "Pagina: 35\n",
      "Pagina: 36\n",
      "Pagina: 37\n",
      "Pagina: 38\n",
      "Pagina: 39\n",
      "Pagina: 40\n",
      "Pagina: 41\n",
      "Pagina: 42\n",
      "Pagina: 43\n",
      "Pagina: 44\n",
      "Pagina: 45\n",
      "Pagina: 46\n",
      "Pagina: 47\n",
      "Pagina: 48\n",
      "Pagina: 49\n",
      "Pagina: 50\n",
      "Pagina: 51\n",
      "Pagina: 52\n",
      "Pagina: 53\n",
      "Pagina: 54\n",
      "Pagina: 55\n",
      "Pagina: 56\n",
      "Pagina: 57\n",
      "Pagina: 58\n",
      "Pagina: 59\n",
      "Pagina: 60\n",
      "Pagina: 61\n",
      "Pagina: 62\n",
      "Pagina: 63\n",
      "Pagina: 64\n",
      "Pagina: 65\n",
      "Pagina: 66\n",
      "Pagina: 67\n",
      "Pagina: 68\n",
      "Pagina: 69\n",
      "Pagina: 70\n",
      "Pagina: 71\n",
      "Pagina: 72\n",
      "Pagina: 73\n",
      "Pagina: 74\n",
      "Pagina: 75\n",
      "Pagina: 76\n",
      "Pagina: 77\n",
      "Pagina: 78\n",
      "Pagina: 79\n",
      "Pagina: 80\n",
      "Pagina: 81\n",
      "Pagina: 82\n",
      "Pagina: 83\n",
      "Pagina: 84\n",
      "Pagina: 85\n",
      "Pagina: 86\n",
      "Pagina: 87\n",
      "Pagina: 88\n",
      "Pagina: 89\n",
      "Pagina: 90\n",
      "Pagina: 91\n",
      "Pagina: 92\n",
      "Pagina: 93\n",
      "Pagina: 94\n",
      "Pagina: 95\n",
      "Pagina: 96\n",
      "Pagina: 97\n",
      "Pagina: 98\n",
      "Pagina: 99\n",
      "Pagina: 100\n",
      "Pagina: 101\n",
      "Pagina: 102\n",
      "Pagina: 103\n",
      "Pagina: 104\n",
      "Pagina: 105\n",
      "Pagina: 106\n",
      "Pagina: 107\n",
      "Pagina: 108\n",
      "Pagina: 109\n",
      "Pagina: 110\n",
      "Pagina: 111\n",
      "Pagina: 112\n",
      "Pagina: 113\n",
      "Pagina: 114\n",
      "Pagina: 115\n",
      "Pagina: 116\n",
      "Pagina: 117\n",
      "Pagina: 118\n",
      "Pagina: 119\n",
      "Pagina: 120\n",
      "Pagina: 121\n",
      "Pagina: 122\n",
      "Pagina: 123\n",
      "Pagina: 124\n",
      "Pagina: 125\n",
      "Pagina: 126\n",
      "Pagina: 127\n",
      "Pagina: 128\n",
      "Pagina: 129\n",
      "Pagina: 130\n",
      "Pagina: 131\n",
      "Pagina: 132\n",
      "Pagina: 133\n",
      "Pagina: 134\n",
      "Pagina: 135\n",
      "Pagina: 136\n",
      "Pagina: 137\n",
      "Pagina: 138\n",
      "Pagina: 139\n",
      "Pagina: 140\n",
      "Pagina: 141\n",
      "Pagina: 142\n",
      "Pagina: 143\n",
      "Pagina: 144\n",
      "Pagina: 145\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "request denied\n",
      "407\n",
      "Pagina: 146\n",
      "request denied\n",
      "407\n",
      "Paramos de recuperar páginas, la última no tiene datos\n",
      "Fin Scrapeo e iniciamos escritura en fichero\n",
      "       Instanciamos el ficehro de salida\n",
      "       Iniciamos el recorrido del Loop\n",
      "Guardando.... 4350  filas\n",
      "Fin de PROCESO\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciamos Scrapeo\")\n",
    "scrape_datos = scrape_motos()\n",
    "print(\"Fin Scrapeo e iniciamos escritura en fichero\")\n",
    "escribimos_excel(scrape_datos)\n",
    "print(\"Fin de PROCESO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
